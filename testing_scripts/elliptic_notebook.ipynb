{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukaspwc/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, use_skip=False):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels[0])\n",
    "        self.conv2 = GCNConv(hidden_channels[0], 2)\n",
    "        self.use_skip = use_skip\n",
    "        if self.use_skip:\n",
    "            self.weight = torch.nn.init.xavier_normal_(\n",
    "                torch.nn.Parameter(torch.Tensor(num_node_features, 2))\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        if self.use_skip:\n",
    "            x = F.softmax(x + torch.matmul(data.x, self.weight), dim=-1)\n",
    "        else:\n",
    "            x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "    def embed(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 203769\n"
     ]
    }
   ],
   "source": [
    "# Load Dataframe\n",
    "df_edge = pd.read_csv(\"elliptic/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
    "df_class = pd.read_csv(\"elliptic/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
    "df_features = pd.read_csv(\n",
    "    \"elliptic/elliptic_bitcoin_dataset/elliptic_txs_features.csv\", header=None\n",
    ")\n",
    "\n",
    "# Setting Column name\n",
    "df_features.columns = (\n",
    "    [\"id\", \"time step\"]\n",
    "    + [f\"trans_feat_{i}\" for i in range(93)]\n",
    "    + [f\"agg_feat_{i}\" for i in range(72)]\n",
    ")\n",
    "\n",
    "#print(\"Number of edges: {}\".format(len(df_edge)))\n",
    "all_nodes = list(\n",
    "    set(df_edge[\"txId1\"])\n",
    "    .union(set(df_edge[\"txId2\"]))\n",
    "    .union(set(df_class[\"txId\"]))\n",
    "    .union(set(df_features[\"id\"]))\n",
    ")\n",
    "nodes_df = pd.DataFrame(all_nodes, columns=[\"id\"]).reset_index()\n",
    "print(\"Number of nodes: {}\".format(len(nodes_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge = (\n",
    "    df_edge.join(\n",
    "        nodes_df.rename(columns={\"id\": \"txId1\"}).set_index(\"txId1\"),\n",
    "        on=\"txId1\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "        nodes_df.rename(columns={\"id\": \"txId2\"}).set_index(\"txId2\"),\n",
    "        on=\"txId2\",\n",
    "        how=\"inner\",\n",
    "        rsuffix=\"2\",\n",
    "    )\n",
    "    .drop(columns=[\"txId1\", \"txId2\"])\n",
    "    .rename(columns={\"index\": \"txId1\", \"index2\": \"txId2\"})\n",
    ")\n",
    "#df_edge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = (\n",
    "    df_class.join(\n",
    "        nodes_df.rename(columns={\"id\": \"txId\"}).set_index(\"txId\"),\n",
    "        on=\"txId\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .drop(columns=[\"txId\"])\n",
    "    .rename(columns={\"index\": \"txId\"})[[\"txId\", \"class\"]]\n",
    ")\n",
    "#df_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = (\n",
    "    df_features.join(nodes_df.set_index(\"id\"), on=\"id\", how=\"inner\")\n",
    "    .drop(columns=[\"id\"])\n",
    "    .rename(columns={\"index\": \"id\"})\n",
    ")\n",
    "df_features.head()\n",
    "df_features = df_features[[\"id\"] + list(df_features.drop(columns=[\"id\"]).columns)]\n",
    "#print(f\"df_edge: \\n {df_edge.head()}\")\n",
    "#print(f\"df_class: \\n {df_class.head()}\")\n",
    "#print(f\"df_features: \\n {df_features.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge_time = df_edge.join(\n",
    "    df_features[[\"id\", \"time step\"]].rename(columns={\"id\": \"txId1\"}).set_index(\"txId1\"),\n",
    "    on=\"txId1\",\n",
    "    how=\"left\",\n",
    "    rsuffix=\"1\",\n",
    ").join(\n",
    "    df_features[[\"id\", \"time step\"]].rename(columns={\"id\": \"txId2\"}).set_index(\"txId2\"),\n",
    "    on=\"txId2\",\n",
    "    how=\"left\",\n",
    "    rsuffix=\"2\",\n",
    ")\n",
    "df_edge_time[\"is_time_same\"] = df_edge_time[\"time step\"] == df_edge_time[\"time step2\"]\n",
    "df_edge_time_fin = df_edge_time[[\"txId1\", \"txId2\", \"time step\"]].rename(\n",
    "    columns={\"txId1\": \"source\", \"txId2\": \"target\", \"time step\": \"time\"}\n",
    ")\n",
    "#df_edge_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.drop(columns=[\"time step\"]).to_csv(\n",
    "    \"elliptic/elliptic_bitcoin_dataset_cont/elliptic_txs_features.csv\",\n",
    "    index=False,\n",
    "    header=None,\n",
    ")\n",
    "df_class.rename(columns={\"txId\": \"nid\", \"class\": \"label\"})[\n",
    "    [\"nid\", \"label\"]\n",
    "].sort_values(by=\"nid\").to_csv(\n",
    "    \"elliptic/elliptic_bitcoin_dataset_cont/elliptic_txs_classes.csv\",\n",
    "    index=False,\n",
    "    header=None,\n",
    ")\n",
    "df_features[[\"id\", \"time step\"]].rename(columns={\"id\": \"nid\", \"time step\": \"time\"})[\n",
    "    [\"nid\", \"time\"]\n",
    "].sort_values(by=\"nid\").to_csv(\n",
    "    \"elliptic/elliptic_bitcoin_dataset_cont/elliptic_txs_nodetime.csv\",\n",
    "    index=False,\n",
    "    header=None,\n",
    ")\n",
    "df_edge_time_fin[[\"source\", \"target\", \"time\"]].to_csv(\n",
    "    \"elliptic/elliptic_bitcoin_dataset_cont/elliptic_txs_edgelist_timed.csv\",\n",
    "    index=False,\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label = (\n",
    "    df_class.rename(columns={\"txId\": \"nid\", \"class\": \"label\"})[[\"nid\", \"label\"]]\n",
    "    .sort_values(by=\"nid\")\n",
    "    .merge(\n",
    "        df_features[[\"id\", \"time step\"]].rename(\n",
    "            columns={\"id\": \"nid\", \"time step\": \"time\"}\n",
    "        ),\n",
    "        on=\"nid\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "node_label[\"label\"] = (\n",
    "    node_label[\"label\"].apply(lambda x: \"3\" if x == \"unknown\" else x).astype(int) - 1\n",
    ")\n",
    "#print(f\"node_label: \\n {node_label.head()}\")\n",
    "node_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nodes_df = node_label.merge(\n",
    "    df_features.rename(columns={\"id\": \"nid\", \"time step\": \"time\"}).drop(\n",
    "        columns=[\"time\"]\n",
    "    ),\n",
    "    on=\"nid\",\n",
    "    how=\"left\",\n",
    ")\n",
    "#print(f\"merged_nodes: \\n {merged_nodes_df.head()}\")\n",
    "merged_nodes_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "test_dataset = []\n",
    "for i in range(49):\n",
    "    nodes_df_tmp = merged_nodes_df[merged_nodes_df[\"time\"] == i + 1].reset_index()\n",
    "    nodes_df_tmp[\"index\"] = nodes_df_tmp.index\n",
    "    df_edge_tmp = (\n",
    "        df_edge_time_fin.join(\n",
    "            nodes_df_tmp.rename(columns={\"nid\": \"source\"})[\n",
    "                [\"source\", \"index\"]\n",
    "            ].set_index(\"source\"),\n",
    "            on=\"source\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(\n",
    "            nodes_df_tmp.rename(columns={\"nid\": \"target\"})[\n",
    "                [\"target\", \"index\"]\n",
    "            ].set_index(\"target\"),\n",
    "            on=\"target\",\n",
    "            how=\"inner\",\n",
    "            rsuffix=\"2\",\n",
    "        )\n",
    "        .drop(columns=[\"source\", \"target\"])\n",
    "        .rename(columns={\"index\": \"source\", \"index2\": \"target\"})\n",
    "    )\n",
    "    x = torch.tensor(\n",
    "        np.array(\n",
    "            nodes_df_tmp.sort_values(by=\"index\").drop(columns=[\"index\", \"nid\", \"label\"])\n",
    "        ),\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    edge_index = torch.tensor(\n",
    "        np.array(df_edge_tmp[[\"source\", \"target\"]]).T, dtype=torch.long\n",
    "    )\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    mask = nodes_df_tmp[\"label\"] != 2\n",
    "    y = torch.tensor(np.array(nodes_df_tmp[\"label\"]))\n",
    "\n",
    "    if i + 1 < 35:\n",
    "        data = Data(x=x, edge_index=edge_index, train_mask=mask, y=y)\n",
    "        train_dataset.append(data)\n",
    "    else:\n",
    "        data = Data(x=x, edge_index=edge_index, test_mask=mask, y=y)\n",
    "        test_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(num_node_features=data.num_node_features, hidden_channels=[100])\n",
    "model.to(device)\n",
    "\n",
    "patience = 50\n",
    "lr = 0.01 #Default 0.01\n",
    "weight_decay = 0 #Tested: 5e-4\n",
    "epoches = 1000 #Default: 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.7, 0.3]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train & Eval\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "f1_illicit = []\n",
    "precisions_illicit = []\n",
    "precisions_licit = []\n",
    "recalls_illicit = []\n",
    "accuracies = []\n",
    "iterations = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        _, pred = out[data.train_mask].max(dim=1)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        test_data = data\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 1 == 0: #Default: 50\n",
    "        model.eval()\n",
    "        ys, preds = [], []\n",
    "        val_loss = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "            val_loss += loss.item() * data.num_graphs\n",
    "            _, pred = out[data.test_mask].max(dim=1)\n",
    "            ys.append(data.y[data.test_mask].cpu())\n",
    "            preds.append(pred.cpu())\n",
    "\n",
    "#Compute Losses & Metrics in current epoch\n",
    "        y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "        val_loss /= len(test_loader.dataset)\n",
    "\n",
    "        if1 = f1_score(y, pred, average='binary', pos_label = 0)\n",
    "        precision_illicit = precision_score(y, pred, average='binary', pos_label = 0)\n",
    "        precision_licit = precision_score(y, pred, average='binary', pos_label = 1)\n",
    "        recall_illicit = recall_score(y, pred, average='binary', pos_label = 0)\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "#Append metrics to metrics of past epochs\n",
    "        iterations.append(epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        f1_illicit.append(if1)\n",
    "        precisions_illicit.append(precision_illicit)\n",
    "        precisions_licit.append(precision_licit)\n",
    "        recalls_illicit.append(recall_illicit)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0: #Default: 50\n",
    "            print(\n",
    "                \"Epoch: {:02d}, Train_Loss: {:.4f}, Val_Loss: {:.4f}, Precision_Illicit: {:.4f}, F1_Illicit: {:.4f}, Accuracy: {:.4f}\".format(\n",
    "                    epoch + 1, train_loss, val_loss, precision_illicit, if1, acc\n",
    "                )\n",
    "            )\n",
    "                \n",
    "#Create DataFrames\n",
    "loss_results = pd.DataFrame(columns = ['Iteration', 'Train_Loss', 'Val_Loss'])\n",
    "metrics_results = pd.DataFrame(columns = ['Iteration', 'Precision_Licit', 'Precision_Illicit', 'Recall_Illicit', 'F1_Illicit', 'Accuracy'])\n",
    "\n",
    "#Collect & Store Losses in Loss-Results-DataFrame\n",
    "loss_results['Iteration'] = iterations\n",
    "loss_results['Train_Loss'] = train_losses\n",
    "loss_results['Val_Loss'] = val_losses\n",
    "\n",
    "#Collect & Store Metrics in Metrics-Results-DataFrame\n",
    "metrics_results['Iteration'] = iterations\n",
    "metrics_results['Precision_Illicit'] = precisions_illicit\n",
    "metrics_results['Precision_Licit'] = precisions_licit\n",
    "metrics_results['Recall_Illicit'] = recalls_illicit\n",
    "#metrics_results['Recall_Licit'] = recalls_licit\n",
    "metrics_results['F1_Illicit'] = f1_illicit\n",
    "metrics_results['Accuracy'] = accuracies\n",
    "#Plot Loss-Results-DataFrame Columns vs Iterations Column\n",
    "loss_results.plot(x = 'Iteration', y = ['Train_Loss', 'Val_Loss'])\n",
    "plt.savefig('loss_results.png')\n",
    "\n",
    "#plt.gca().set_color_cycle(['magenta', 'blue', 'green', 'red', 'orange'])\n",
    "matplotlib.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=['tab:purple', 'tab:blue', 'tab:green', 'tab:red', 'tab:orange']) \n",
    "\n",
    "#Plot Metrics-Results-DataFrame Columns vs Iterations Column \n",
    "metrics_results.plot(x = 'Iteration', y = ['Precision_Licit', 'Precision_Illicit', 'Recall_Illicit', 'F1_Illicit', 'Accuracy'])\n",
    "plt.savefig('metrics_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = [1,0,0,1,1]\n",
    "predy = [1,1,0,1,1]\n",
    "pos_label = 0\n",
    "\n",
    "score = precision_score(testy, predy, average='binary', pos_label = pos_label)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "beta_venv",
   "language": "python",
   "name": "beta_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
